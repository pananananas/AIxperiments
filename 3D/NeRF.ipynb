{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    def __init__(self, D=8, W=256, input_ch=15, output_ch=3):  # output_ch is 3 for RGB\n",
    "        super(NeRF, self).__init__()\n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.input_ch = input_ch\n",
    "        self.output_ch = output_ch\n",
    "        \n",
    "        self.layers = [nn.Linear(input_ch, W)]\n",
    "        self.layers += [nn.Linear(W, W) for _ in range(D-1)]\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        self.output_layer = nn.Linear(W, output_ch)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = torch.relu(layer(h))\n",
    "        return self.output_layer(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLFFDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir, 'images_8')\n",
    "        self.images = sorted([os.path.join(self.image_dir, f) for f in os.listdir(self.image_dir) if f.endswith(('.JPG', '.png'))])\n",
    "        self.poses_bounds = np.load(os.path.join(root_dir, 'poses_bounds.npy'))\n",
    "        print(f'Loaded {len(self.images)} images.')\n",
    "        print(f'Camera parameters shape: {self.poses_bounds.shape}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = np.array(Image.open(img_path)) / 255.0\n",
    "        cam_params = self.poses_bounds[idx, :-2]  # Assuming last two values in poses_bounds are bounds\n",
    "        # Flatten the image and duplicate cam_params to match\n",
    "        img = img.reshape(-1, 3)\n",
    "        cam_params = np.tile(cam_params, (img.shape[0], 1))\n",
    "        return torch.tensor(img, dtype=torch.float32), torch.tensor(cam_params, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nerf(nerf, dataset, epochs=100, batch_size=1, lr=5e-4, save_path='nerf_model.pth'):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(nerf.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    nerf.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, cam_params in tqdm(dataloader):\n",
    "            images, cam_params = images.to(device), cam_params.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = nerf(cam_params)\n",
    "            loss = criterion(predictions, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader)}')\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(nerf.state_dict(), save_path)\n",
    "    print(f'Model saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 images.\n",
      "Camera parameters shape: (30, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:53<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.09768241910884777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:48<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.025896312296390535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:47<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.021713684995969137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:45<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.022999316019316516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:48<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.021683336483935514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:54<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.021402562720080218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:46<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.022627480265994867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:44<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.021569070344169935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:45<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.021984835093220075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:47<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.021986890646318594\n",
      "Model saved to nerf_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1  # This should be 1 due to the high resolution of the images\n",
    "LR = 1e-3\n",
    "CHUNK_SIZE = 1024  # Adjust this value as needed\n",
    "\n",
    "# Paths\n",
    "DATASET_DIR = '/Users/ewojcik/Code/AIxperiments/3D/LLFF_Data/real_iconic/airplants'  # Adjust to your dataset directory\n",
    "\n",
    "# Initialize Dataset and Model\n",
    "dataset = LLFFDataset(DATASET_DIR)\n",
    "nerf_model = NeRF()\n",
    "\n",
    "# Train the Model\n",
    "train_nerf(nerf_model, dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_visualize_model(model_path, image_path, cam_params, chunk_size=1024, output_file='output.png'):\n",
    "    # Load the model\n",
    "    nerf_model = NeRF()\n",
    "    nerf_model.load_state_dict(torch.load(model_path))\n",
    "    nerf_model.to(device)\n",
    "    nerf_model.eval()\n",
    "    \n",
    "    # Load the image\n",
    "    img = np.array(Image.open(image_path)) / 255.0\n",
    "    img_flatten = img.reshape(-1, 3)\n",
    "    \n",
    "    # Duplicate the camera parameters\n",
    "    cam_params = np.tile(cam_params, (img_flatten.shape[0], 1))\n",
    "    \n",
    "    # Convert to tensors\n",
    "    cam_params_tensor = torch.tensor(cam_params, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Initialize an empty array for the output image\n",
    "    output_flatten = np.zeros_like(img_flatten)\n",
    "    \n",
    "    # Process the image in chunks\n",
    "    for i in range(0, img_flatten.shape[0], chunk_size):\n",
    "        img_chunk = img_flatten[i:i+chunk_size]\n",
    "        cam_params_chunk = cam_params_tensor[i:i+chunk_size]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = nerf_model(cam_params_chunk).cpu().numpy()\n",
    "        \n",
    "        output_flatten[i:i+chunk_size] = predictions\n",
    "    \n",
    "    # Reshape and save the output image\n",
    "    output_image = (output_flatten * 255).astype(np.uint8).reshape(img.shape)\n",
    "    output_image = Image.fromarray(output_image)\n",
    "    output_image.save(output_file)\n",
    "    print(f'Output image saved as {output_file}')\n",
    "    \n",
    "    # Display the output image\n",
    "    plt.imshow(output_image)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'nerf_model.pth'\n",
    "IMAGE_PATH = '/Users/ewojcik/Code/AIxperiments/3D/LLFF_Data/real_iconic/airplants/images/IMG_2066.JPG'  # Adjust to one of your images\n",
    "CAM_PARAMS = np.load('/Users/ewojcik/Code/AIxperiments/3D/LLFF_Data/real_iconic/airplants/poses_bounds.npy')\n",
    "\n",
    "load_and_visualize_model(MODEL_PATH, IMAGE_PATH, CAM_PARAMS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
