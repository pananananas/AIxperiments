{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "\n",
    "from depth_anything.dpt import DepthAnything\n",
    "from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/Users/ewojcik/Code/AIxperiments/Images/imgs'\n",
    "outdir = './output'\n",
    "encoder = 'vitl'  # choices: 'vits', 'vitb', 'vitl'\n",
    "pred_only = False\n",
    "grayscale = False\n",
    "\n",
    "margin_width = 50\n",
    "caption_height = 60\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_thickness = 2\n",
    "\n",
    "DEVICE = 'mps'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/depth_anything_py310/lib/python3.10/site-packages/huggingface_hub/hub_mixin.py:824: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_file, map_location=torch.device(map_location))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 335.32M\n"
     ]
    }
   ],
   "source": [
    "depth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_{}14'.format(encoder)).to(DEVICE).eval()\n",
    "\n",
    "total_params = sum(param.numel() for param in depth_anything.parameters())\n",
    "print('Total parameters: {:.2f}M'.format(total_params / 1e6))\n",
    "\n",
    "transform = Compose([\n",
    "    Resize(\n",
    "        width=518,\n",
    "        height=518,\n",
    "        resize_target=False,\n",
    "        keep_aspect_ratio=True,\n",
    "        ensure_multiple_of=14,\n",
    "        resize_method='lower_bound',\n",
    "        image_interpolation_method=cv2.INTER_CUBIC,\n",
    "    ),\n",
    "    NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    PrepareForNet(),\n",
    "])\n",
    "\n",
    "if os.path.isfile(img_path):\n",
    "    if img_path.endswith('txt'):\n",
    "        with open(img_path, 'r') as f:\n",
    "            filenames = f.read().splitlines()\n",
    "    else:\n",
    "        filenames = [img_path]\n",
    "else:\n",
    "    filenames = os.listdir(img_path)\n",
    "    filenames = [os.path.join(img_path, filename) for filename in filenames if not filename.startswith('.')]\n",
    "    filenames.sort()\n",
    "\n",
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:00<00:05,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for CNV000001-2.jpg: 0.7220 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:01<00:04,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for CNV000001-3.jpg: 0.6936 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:02<00:03,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for CNV000001.jpg: 0.6578 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:02<00:02,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for CNV000002.jpg: 0.6695 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:03<00:02,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for CNV000003.jpg: 0.6588 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:04<00:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for CNV000010.jpg: 0.6512 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:04<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for Untitled-1.jpg: 0.6629 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for img.jpg: 0.6592 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for filename in tqdm(filenames):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    raw_image = cv2.imread(filename)\n",
    "    image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    \n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    image = transform({'image': image})['image']\n",
    "    image = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        depth = depth_anything(image)\n",
    "    \n",
    "    depth = F.interpolate(depth[None], (h, w), mode='bilinear', align_corners=False)[0, 0]\n",
    "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "    \n",
    "    depth = depth.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    if grayscale:\n",
    "        depth = np.repeat(depth[..., np.newaxis], 3, axis=-1)\n",
    "    else:\n",
    "        depth = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)\n",
    "    \n",
    "    filename = os.path.basename(filename)\n",
    "    \n",
    "    if pred_only:\n",
    "        cv2.imwrite(os.path.join(outdir, filename[:filename.rfind('.')] + '_depth.png'), depth)\n",
    "    else:\n",
    "        split_region = np.ones((raw_image.shape[0], margin_width, 3), dtype=np.uint8) * 255\n",
    "        combined_results = cv2.hconcat([raw_image, split_region, depth])\n",
    "        \n",
    "        caption_space = np.ones((caption_height, combined_results.shape[1], 3), dtype=np.uint8) * 255\n",
    "        captions = ['Raw image', 'Depth Anything']\n",
    "        segment_width = w + margin_width\n",
    "        \n",
    "        for i, caption in enumerate(captions):\n",
    "            # Calculate text size\n",
    "            text_size = cv2.getTextSize(caption, font, font_scale, font_thickness)[0]\n",
    "\n",
    "            # Calculate x-coordinate to center the text\n",
    "            text_x = int((segment_width * i) + (w - text_size[0]) / 2)\n",
    "\n",
    "            # Add text caption\n",
    "            cv2.putText(caption_space, caption, (text_x, 40), font, font_scale, (0, 0, 0), font_thickness)\n",
    "        \n",
    "        final_result = cv2.vconcat([caption_space, combined_results])\n",
    "        \n",
    "        cv2.imwrite(os.path.join(outdir, filename[:filename.rfind('.')] + '_img_depth.png'), final_result)\n",
    "    end_time = time.time()  # End timing\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Processing time for {filename}: {execution_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth_anything_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
